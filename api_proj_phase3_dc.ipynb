{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Phase 2 - Data Cleaning and EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# import tensorflow_hub as hub\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "from nltk import tokenize\n",
    "import html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all resource files\n",
    "path = 'dcbox resource/'\n",
    "file = 'business.json'\n",
    "dest_path = 'newest dcbox resource/'\n",
    "dirs = os.listdir(path)\n",
    "dirs.remove('.ipynb_checkpoints')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitness sports.json\n",
      "media enterainment.json\n",
      "pet.json\n",
      "lifecycle home.json\n",
      "software tech.json\n",
      "business.json\n",
      "children education.json\n",
      "fashion beauty.json\n",
      "food beverage.json\n",
      "green cleantech.json\n",
      "consumer.json\n",
      "healthcare.json\n"
     ]
    }
   ],
   "source": [
    "# Load paragraph body only\n",
    "for file in dirs:\n",
    "    print(file)\n",
    "    body_df = pd.read_json(path + file).body\n",
    "    sentences = []\n",
    "    for each_article in body_df:\n",
    "        each_article = html.unescape(each_article)\n",
    "        each_article = each_article.replace('|', '')\n",
    "        each_article = each_article.replace('? ?', '')\n",
    "        # remove phone number\n",
    "        each_article = re.sub(r'((1-\\d{3}-\\d{3}-\\d{4})|(\\(\\d{3}\\) \\d{3}-\\d{4})|(\\d{3}-\\d{3}-\\d{4}))$', '', each_article);\n",
    "        # clean head with Client-specific Styles\n",
    "        if \"Client-specific Styles\" in each_article:\n",
    "            if \"Dear \" in each_article or \"Dear,\" in each_article:\n",
    "                each_article = each_article[each_article.find(\"Dear\"):]\n",
    "            elif \"Hi \" in each_article or \"Hi,\" in each_article:\n",
    "                each_article = each_article[each_article.find(\"Hi\"):]\n",
    "            elif \"Hello \" in each_article or \"Hello,\" in each_article:\n",
    "                each_article = each_article[each_article.find(\"Hello\"):]\n",
    "            elif \"On my weekly\" in each_article:\n",
    "                each_article = each_article[each_article.find(\"On my weekly\"):]\n",
    "            elif \"  News from\" in each_article:\n",
    "                each_article = each_article[each_article.find(\"  News from\")+2:]\n",
    "            elif \"  This week\" in each_article:\n",
    "                each_article = each_article[each_article.find(\"  This week\")+2:]\n",
    "            elif \"margin-bottom: 1em; }\" in each_article:\n",
    "                each_article = each_article[each_article.rfind(\"margin-bottom: 1em; }\"):]\n",
    "            elif \"overflow:hidden; }\" in each_article:\n",
    "                each_article = each_article[each_article.find(\"overflow:hidden; }\"):]\n",
    "            elif \"border: none; }\" in each_article:\n",
    "                each_article = each_article[each_article.find(\"border: none; }\"):]\n",
    "        elif \"Newsletter #\" in each_article:\n",
    "            each_article = each_article[each_article.find(\"Newsletter # \")+len(\"Newsletter # \"):]\n",
    "        # clean tail with \"read more\"\n",
    "        if \". Read more\" in each_article:\n",
    "            # small probelm, replace is fine\n",
    "            if \"Read more Here.\" in each_article:\n",
    "                each_article = each_article.replace(\"Read more Here.\", '')\n",
    "            if \"Read more [here.]\" in each_article:\n",
    "                each_article = each_article.replace(\"Read more [here.]\", '')\n",
    "            if \"Read more[ here.]\" in each_article:\n",
    "                each_article = each_article.replace(\"Read more[ here.]\", '')\n",
    "            # cut the tails\n",
    "            if \"Read more\" in each_article:\n",
    "                each_article = each_article[:each_article.find(\"Read more\")]\n",
    "        elif \"As always, I\" in each_article or \"As always, my\" in each_article or \"As always, thank\" in each_article or \"As always, feel\" in each_article:\n",
    "            each_article = each_article[:each_article.find(\"As always,\")]\n",
    "        elif \"Follow Me On\" in each_article:\n",
    "            each_article = each_article[:each_article.find(\"Follow Me On\")]\n",
    "        elif \"Please feel\" in each_article:\n",
    "            each_article = each_article[:each_article.find(\"Please feel\")]\n",
    "        elif \". Feel free to\" in each_article:\n",
    "            each_article = each_article[:each_article.find(\" Feel free to\")]\n",
    "        elif \"For more updates\" in each_article:\n",
    "            each_article = each_article[:each_article.find(\"For more updates\")]\n",
    "        elif \"Learn more .\" in each_article:\n",
    "            each_article = each_article[:each_article.find(\"Learn more .\")]\n",
    "        elif \"  Follow Me\" in each_article:\n",
    "            each_article = each_article[:each_article.find(\"  Follow Me\")]\n",
    "        elif \". To stay up-to-date\" in each_article or \"  To stay up-to-date\" in each_article or \"To stay up to date\" in each_article:\n",
    "            each_article = each_article[:each_article.find(\"To stay up\")]\n",
    "        elif \"  Follow Me\" in each_article:\n",
    "            each_article = each_article[:each_article.find(\"  Follow Me\")]\n",
    "        elif \"Thank you for the opportunity\" in each_article:\n",
    "            each_article = each_article[:each_article.find(\"Thank you for the opportunity\")]\n",
    "        elif \"All are encouraged \" in each_article:\n",
    "            each_article = each_article[:each_article.find(\"All are encouraged \")]\n",
    "        elif \"Unsubscribe\" in each_article:\n",
    "            each_article = each_article[:each_article.find(\"Unsubscribe\")]\n",
    "        elif \"Have a great\" in each_article:\n",
    "            each_article = each_article[:each_article.find(\"Have a great\")]\n",
    "        elif \"Remember to keep\" in each_article:\n",
    "            each_article = each_article[:each_article.find(\"Remember to keep\")]\n",
    "        for each in tokenize.sent_tokenize(each_article)[:-2]:\n",
    "            if len(each) > 20:\n",
    "                sentences.append(each)\n",
    "    sentences_df = pd.DataFrame(sentences, columns=['sentences'])\n",
    "    sentences_df.to_csv(dest_path + file + \".csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
